{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequistes  \r\n",
        "\r\n",
        "## Azure Key Vault \r\n",
        "1. Azure Key Vault stores Twitter developer account scretes: TWEET-CONSUMER-KEY, TWEET-CONSUMER-SECRET, TWEET-ACCESS-TOKEN, TWEET-ACCESS-SECRET\r\n",
        "1. Add Azure Key Vault Linked Service \"mylsKeyVault\"\r\n",
        "1. Add AKV access policy, secrete 'Get' permission"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Primary storage info\r\n",
        "account_name = 'twitterhashtagsandbox' # fill in your primary account name\r\n",
        "hashtag = '#AzureDataFactory'\r\n",
        "container_name = 'workspace' # fill in your primary file system name"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "from notebookutils import mssparkutils\r\n",
        "from datetime import datetime, timedelta\r\n",
        "import tweepy as tw\r\n",
        "\r\n",
        "relative_path = 'myfolder' # fill in your relative folder path\r\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\r\n",
        "\r\n",
        "akv_linked_serivce = \"mylsKeyVault\"\r\n",
        "\r\n",
        "auth = tw.OAuthHandler(mssparkutils.credentials.getSecretWithLS(akv_linked_serivce, \"TWEET-CONSUMER-KEY\"),mssparkutils.credentials.getSecretWithLS(akv_linked_serivce, \"TWEET-CONSUMER-SECRET\") )\r\n",
        "auth.set_access_token(mssparkutils.credentials.getSecretWithLS(akv_linked_serivce, \"TWEET-ACCESS-TOKEN\"), mssparkutils.credentials.getSecretWithLS(akv_linked_serivce, \"TWEET-ACCESS-SECRET\") )\r\n",
        "api = tw.API(auth, wait_on_rate_limit=True)\r\n",
        "\r\n",
        "all_tweets = []\r\n",
        " \r\n",
        "#main function that does all the work\r\n",
        "def twitterQuery(search_words, all_tweets): \r\n",
        "    tweets = tw.Cursor(api.search, q=search_words).items()\r\n",
        "        \r\n",
        "    #potential complexity issue. the more tweet.attributes you pull the exponentially longer this takes\r\n",
        "    hashtag_query = [[tweet.text, tweet.user.screen_name,tweet.user.location, tweet.created_at, tweet.retweet_count, tweet.favorite_count, tweet.entities['urls']] for tweet in tweets]\r\n",
        "    #print(hashtag_query)\r\n",
        "\r\n",
        "    for i in hashtag_query:\r\n",
        "        url_status = 0\r\n",
        "        i[3] = i[3].strftime('%Y-%m-%d')\r\n",
        "        if i[6] != []:\r\n",
        "            for j in range(len(i[6])):\r\n",
        "                if i[6][j]['display_url']=='twitter.com/i/web/status/1â€¦':\r\n",
        "                    i[6] = i[6][j]['expanded_url']\r\n",
        "                    url_status = 1\r\n",
        "        if url_status == 0:\r\n",
        "            i[6] = 'Please find it manually'\r\n",
        "\r\n",
        "    all_tweets.extend(hashtag_query)\r\n",
        " \r\n",
        "#hashtags to pass into main method\r\n",
        "#hashtags = [\"#DataFactory\", \"#AzureDataFactory\"]\r\n",
        "hashtags = [hashtag]\r\n",
        "\r\n",
        "for i in hashtags:\r\n",
        "    twitterQuery(i+\" -filter:retweets\", all_tweets)\r\n",
        "\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "today = datetime.today()\r\n",
        "csv_path = adls_path + '/' + today.strftime(\"%Y%m%d\")\r\n",
        "print(csv_path)\r\n",
        "\r\n",
        "df = spark.createDataFrame(all_tweets, ['tweet','user', 'location', 'created_at', 'retweet_count', 'favorites_count', 'link'])\r\n",
        "df.orderBy([(\"created_at\")], ascending=[0]) \\\r\n",
        "    .coalesce(1) \\\r\n",
        "    .dropDuplicates(subset = ['tweet','user']) \\\r\n",
        "    .coalesce(1).write.csv(csv_path, mode = 'overwrite', header = 'true')\r\n"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}